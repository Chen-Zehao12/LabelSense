{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = '25%*3'\n",
    "\n",
    "with open('origin/topics.rbb') as f:\n",
    "    label_desc_raw = f.readlines()\n",
    "    label_desc = dict()\n",
    "    for line in label_desc_raw:\n",
    "        if '      ' in line:\n",
    "            line = line.strip().split('      ')\n",
    "        elif '     ' in line:\n",
    "            line = line.strip().split('     ')\n",
    "        elif '    ' in line:\n",
    "            line = line.strip().split('    ')\n",
    "        label_desc[line[0]] = line[1]\n",
    "\n",
    "with open('label-description.jsonl', 'w') as f:\n",
    "    for k, v in label_desc.items():\n",
    "        json.dump({'label': k, 'description': v}, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('origin/rcv1-v2.topics.qrels') as f:\n",
    "    topic_did = f.readlines()\n",
    "    topic_did = list(map(lambda x: x.strip().split(' '), topic_did))\n",
    "    topic_did_dict = defaultdict(list)\n",
    "    for label, did, _ in topic_did:\n",
    "        topic_did_dict[did].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    ret_data = []\n",
    "    piece = {'text': ''}\n",
    "    for line in data:\n",
    "        if re.findall('\\.I.(.*?)\\n', line):\n",
    "            if piece['text']:\n",
    "                piece['text'] = piece['text'].strip()\n",
    "                ret_data.append(piece)\n",
    "                piece = {'text': ''}\n",
    "            piece['labels'] = topic_did_dict[re.findall('\\.I.(.*?)\\n', line)[0]]\n",
    "            continue\n",
    "        if re.findall('\\.W\\n', line):\n",
    "            continue\n",
    "        piece['text'] += line.strip() + ' '\n",
    "    \n",
    "    if piece['text']:\n",
    "        piece['text'] = piece['text'].strip()\n",
    "        ret_data.append(piece)\n",
    "\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('origin/lyrl2004_tokens_train.dat') as f:\n",
    "    train = f.readlines()\n",
    "    train = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for pt in ['pt0', 'pt1', 'pt2', 'pt3']:\n",
    "    with open(f'origin/lyrl2004_tokens_test_{pt}.dat') as f:\n",
    "        test += f.readlines()\n",
    "test = load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete \\n and \\\n",
    "for line in train:\n",
    "    line['text'] = line['text'].replace('\\n', ' ')\n",
    "    line['text'] = line['text'].replace('\\\\', '')\n",
    "\n",
    "for line in test:\n",
    "    line['text'] = line['text'].replace('\\n', ' ')\n",
    "    line['text'] = line['text'].replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count classes\n",
    "train_class_count = defaultdict(int)\n",
    "test_class_count = defaultdict(int)\n",
    "\n",
    "for line in train:\n",
    "    for label in line['labels']:\n",
    "        train_class_count[label] += 1\n",
    "\n",
    "for line in test:\n",
    "    for label in line['labels']:\n",
    "        test_class_count[label] += 1\n",
    "\n",
    "unseen_class_count = train_class_count.keys() - test_class_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 23149\n",
      "Number of test data: 781265\n",
      "Number of classes in train data: 101\n",
      "Number of classes in test data: 103\n",
      "Number of unseen classes: 0\n",
      "Count of train label:\n",
      " [('CCAT', 10786), ('GCAT', 6970), ('MCAT', 5882), ('C15', 4179), ('ECAT', 3449), ('M14', 2541), ('C151', 2366), ('C152', 1930), ('GPOL', 1647), ('M13', 1596), ('M141', 1508), ('C18', 1462), ('M11', 1294), ('E21', 1255), ('C181', 1205), ('C17', 1172), ('GCRIM', 1133), ('GVIO', 1115), ('C31', 1058), ('GDIP', 1004), ('C13', 947), ('M131', 943), ('C24', 922), ('GSPO', 913), ('E212', 853), ('C21', 793), ('M12', 732), ('M132', 699), ('E12', 679), ('C11', 674), ('E51', 641), ('M143', 606), ('GJOB', 471), ('E41', 449), ('C33', 443), ('C171', 437), ('E211', 407), ('E512', 400), ('C1511', 399), ('C12', 381), ('G15', 363), ('GVOTE', 346), ('C42', 343), ('C41', 312), ('M142', 311), ('GDIS', 293), ('C411', 286), ('C172', 285), ('E11', 279), ('C174', 246), ('GDEF', 233), ('C183', 202), ('GHEA', 197), ('C312', 196), ('C22', 190), ('E13', 187), ('GENV', 172), ('E131', 167), ('C311', 166), ('E71', 166), ('GPRO', 166), ('C14', 160), ('C182', 142), ('G154', 138), ('GWEA', 135), ('C34', 120), ('GENT', 106), ('E511', 102), ('E121', 94), ('GREL', 92), ('GODD', 90), ('C173', 76), ('E31', 66), ('E14', 65), ('C23', 62), ('G153', 59), ('E513', 54), ('G158', 52), ('GWELF', 51), ('C16', 49), ('G151', 49), ('G157', 45), ('C32', 43), ('E311', 43), ('C313', 41), ('E411', 40), ('G155', 38), ('GSCI', 37), ('G152', 35), ('E143', 34), ('C331', 31), ('GTOUR', 23), ('E132', 17), ('E61', 15), ('GOBIT', 13), ('E141', 12), ('E142', 8), ('GFAS', 6), ('E313', 3), ('G159', 2), ('G156', 2)]\n",
      "Count of test label:\n",
      " [('CCAT', 370541), ('GCAT', 232297), ('MCAT', 198938), ('C15', 147606), ('ECAT', 116471), ('M14', 82899), ('C151', 79524), ('C152', 71162), ('GPOL', 55231), ('M13', 52038), ('C18', 51355), ('M11', 47402), ('M141', 46200), ('C181', 42169), ('E21', 41875), ('C17', 40983), ('C31', 39451), ('GDIP', 36735), ('C13', 36463), ('GSPO', 34404), ('GVIO', 31500), ('C24', 31231), ('GCRIM', 31086), ('M131', 27242), ('E212', 26552), ('E12', 26421), ('M132', 26053), ('M12', 25304), ('C21', 24610), ('C11', 23651), ('C1511', 22812), ('M143', 21351), ('E51', 20639), ('G15', 20309), ('C171', 17876), ('GJOB', 16770), ('E41', 16586), ('E211', 15361), ('C33', 14889), ('E512', 12234), ('M142', 11819), ('C12', 11563), ('C42', 11535), ('C172', 11202), ('GVOTE', 11186), ('C41', 11043), ('C411', 9986), ('GDEF', 8609), ('GDIS', 8364), ('E11', 8289), ('G154', 8266), ('C14', 7250), ('C183', 7204), ('C312', 6452), ('E13', 6416), ('GENV', 6089), ('C22', 5929), ('GHEA', 5833), ('C174', 5625), ('E131', 5492), ('GPRO', 5332), ('E71', 5102), ('C34', 4715), ('C182', 4529), ('G158', 4248), ('C311', 4133), ('GWEA', 3743), ('GENT', 3695), ('G151', 3258), ('E511', 2831), ('GREL', 2757), ('GODD', 2712), ('C23', 2563), ('C173', 2560), ('GSCI', 2373), ('E31', 2349), ('G153', 2301), ('E513', 2236), ('E14', 2112), ('E411', 2096), ('E121', 2088), ('G155', 2086), ('G152', 2072), ('C32', 2041), ('G157', 1991), ('C16', 1871), ('GWELF', 1818), ('E311', 1658), ('C331', 1179), ('E143', 1172), ('C313', 1074), ('E132', 922), ('GOBIT', 831), ('GTOUR', 657), ('E61', 376), ('E141', 364), ('GFAS', 307), ('G156', 258), ('E142', 192), ('E313', 108), ('E312', 52), ('G159', 38), ('GMIL', 5)]\n"
     ]
    }
   ],
   "source": [
    "print('Number of train data:', len(train))\n",
    "print('Number of test data:', len(test))\n",
    "\n",
    "print('Number of classes in train data:', len(train_class_count))\n",
    "print('Number of classes in test data:', len(test_class_count))\n",
    "print('Number of unseen classes:', len(unseen_class_count))\n",
    "\n",
    "print('Count of train label:\\n', sorted(train_class_count.items(), key=lambda x: -x[1]))\n",
    "print('Count of test label:\\n', sorted(test_class_count.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length of train text: 746.1848459976673\n",
      "Avg length of test text: 764.0679462154327\n"
     ]
    }
   ],
   "source": [
    "train_text_length, test_text_length = 0, 0\n",
    "for line in train:\n",
    "    train_text_length += len(line['text'])\n",
    "for line in test:\n",
    "    test_text_length += len(line['text'])\n",
    "\n",
    "print('Avg length of train text:', train_text_length / len(train))\n",
    "print('Avg length of test text:', test_text_length / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2id\n",
    "label2id = dict()\n",
    "cnt = 0\n",
    "\n",
    "for line in train:\n",
    "    for label in line['labels']:\n",
    "        if label not in label2id:\n",
    "            label2id[label] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "for line in test:\n",
    "    for label in line['labels']:\n",
    "        if label not in label2id:\n",
    "            label2id[label] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to one label, labels[0] is the rarest one\n",
    "for line in train:\n",
    "    minn, minn_label = 20000, ''\n",
    "    for label in line['labels']:\n",
    "        if train_class_count[label] < minn:\n",
    "            minn = train_class_count[label]\n",
    "            minn_label = label\n",
    "    if line['labels'][0] != minn_label:\n",
    "        line['labels'].remove(minn_label)\n",
    "        line['labels'].insert(0, minn_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete long tail data\n",
    "label_collection = [[] for _ in range(len(train_class_count))]\n",
    "for line in train:\n",
    "    idx = label2id[line['labels'][0]]\n",
    "    label_collection[idx].append(line)\n",
    "\n",
    "if state == '50%*1':\n",
    "    threshold = int(np.percentile([len(t) for t in label_collection], 50))\n",
    "    label_collection = [t[:threshold * 1] for t in label_collection if len(t) >= threshold]\n",
    "elif state == '25%*3':\n",
    "    threshold = int(np.percentile([len(t) for t in label_collection], 25))\n",
    "    label_collection = [t[:threshold * 3] for t in label_collection if len(t) >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh label2id\n",
    "label2id = dict()\n",
    "for idx, piece in enumerate(label_collection):\n",
    "    label2id[piece[0]['labels'][0]] = idx\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unseen test\n",
    "idx = 0\n",
    "while idx < len(test):\n",
    "    for label in test[idx]['labels']:\n",
    "        if label not in label2id:\n",
    "            test.pop(idx)\n",
    "            break\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{state}/id2label.json', 'w') as f:\n",
    "    json.dump(id2label, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f'{state}/label2id.json', 'w') as f:\n",
    "    json.dump(label2id, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle train dataset\n",
    "collection_pointer = [0] * len(label_collection)\n",
    "\n",
    "shuffle_train = []\n",
    "global_idx, idx = 0, 0\n",
    "while global_idx < sum(len(t) for t in label_collection):\n",
    "    if collection_pointer[idx] < len(label_collection[idx]):\n",
    "        shuffle_train.append(label_collection[idx][collection_pointer[idx]])\n",
    "        collection_pointer[idx] += 1\n",
    "        idx = (idx + 1) % len(label_collection)\n",
    "        global_idx += 1\n",
    "    else:\n",
    "        idx = (idx + 1) % len(label_collection) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of test\n",
    "test = test[:len(shuffle_train) * 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 6465\n",
      "Number of test data: 19395\n"
     ]
    }
   ],
   "source": [
    "print('Number of train data:', len(shuffle_train))\n",
    "print('Number of test data:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unseen train labels\n",
    "for line in shuffle_train:\n",
    "    idx = 0\n",
    "    while idx < len(line['labels']):\n",
    "        if line['labels'][idx] not in label2id:\n",
    "            line['labels'].pop(idx)\n",
    "        else:\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 6465\n",
      "Number of test data: 19395\n",
      "Number of classes in train data: 76\n",
      "Number of classes in test data: 25\n",
      "Number of unseen classes: 51\n",
      "Count of train label:\n",
      " [('GCAT', 2522), ('M14', 628), ('GPOL', 545), ('C31', 502), ('C13', 495), ('E51', 402), ('E12', 388), ('GCRIM', 375), ('G15', 338), ('C24', 305), ('M141', 299), ('GDIP', 291), ('C181', 282), ('C151', 255), ('C21', 237), ('GVIO', 228), ('E212', 220), ('E41', 219), ('C152', 214), ('M132', 213), ('C11', 197), ('GDIS', 194), ('M131', 183), ('M11', 178), ('E512', 177), ('E211', 176), ('GHEA', 163), ('C171', 162), ('C12', 153), ('E11', 139), ('C33', 139), ('GPRO', 138), ('C312', 134), ('M12', 129), ('G154', 129), ('M143', 124), ('GENV', 121), ('C34', 120), ('GVOTE', 119), ('C22', 118), ('C42', 118), ('E131', 116), ('GDEF', 111), ('C311', 110), ('M142', 108), ('C183', 108), ('C182', 107), ('C172', 105), ('GENT', 103), ('C411', 103), ('E71', 103), ('C174', 101), ('C1511', 101), ('GSPO', 101), ('GWEA', 101), ('C14', 99), ('E121', 93), ('GREL', 92), ('E511', 91), ('GODD', 89), ('C173', 76), ('C23', 62), ('G153', 59), ('E513', 54), ('GWELF', 50), ('C16', 49), ('G158', 46), ('G157', 45), ('C32', 43), ('G151', 43), ('E311', 43), ('C313', 41), ('E411', 40), ('GSCI', 37), ('G152', 35), ('E143', 33)]\n",
      "Count of test label:\n",
      " [('GCAT', 19395), ('GPOL', 5072), ('GVIO', 4044), ('GDIP', 3896), ('GSPO', 3682), ('GCRIM', 2118), ('GVOTE', 1572), ('GDEF', 843), ('GPRO', 598), ('GDIS', 592), ('G15', 439), ('GREL', 370), ('GENV', 338), ('GHEA', 334), ('GENT', 323), ('GWEA', 277), ('GODD', 227), ('G158', 165), ('GSCI', 165), ('GWELF', 46), ('G154', 42), ('G151', 31), ('G152', 15), ('G153', 6), ('G157', 1)]\n",
      "Avg length of train text: 829.6017014694509\n",
      "Avg length of test text: 1098.5903583397783\n"
     ]
    }
   ],
   "source": [
    "# count classes\n",
    "train_class_count = defaultdict(int)\n",
    "test_class_count = defaultdict(int)\n",
    "\n",
    "for line in shuffle_train:\n",
    "    for label in line['labels']:\n",
    "        train_class_count[label] += 1\n",
    "\n",
    "for line in test:\n",
    "    for label in line['labels']:\n",
    "        test_class_count[label] += 1\n",
    "\n",
    "unseen_class_count = train_class_count.keys() - test_class_count.keys()\n",
    "print('Number of train data:', len(shuffle_train))\n",
    "print('Number of test data:', len(test))\n",
    "\n",
    "print('Number of classes in train data:', len(train_class_count))\n",
    "print('Number of classes in test data:', len(test_class_count))\n",
    "print('Number of unseen classes:', len(unseen_class_count))\n",
    "\n",
    "print('Count of train label:\\n', sorted(train_class_count.items(), key=lambda x: -x[1]))\n",
    "print('Count of test label:\\n', sorted(test_class_count.items(), key=lambda x: -x[1]))\n",
    "\n",
    "train_text_length, test_text_length = 0, 0\n",
    "for line in shuffle_train:\n",
    "    train_text_length += len(line['text'])\n",
    "for line in test:\n",
    "    test_text_length += len(line['text'])\n",
    "\n",
    "print('Avg length of train text:', train_text_length / len(shuffle_train))\n",
    "print('Avg length of test text:', test_text_length / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label description\n",
    "with open('label-description.jsonl') as f:\n",
    "    label_desc = f.readlines()\n",
    "    label_desc = list(map(json.loads, label_desc))\n",
    "    label_desc = {line['label']: line['description'] for line in label_desc}\n",
    "\n",
    "for line in shuffle_train:\n",
    "    label_text = line['labels'][0]\n",
    "    for i, label in enumerate(line['labels']):\n",
    "        line['labels'][i] = label2id[label]\n",
    "    line['label_description'] = label_desc[label_text]\n",
    "\n",
    "for line in test:\n",
    "    label_text = line['labels'][0]\n",
    "    for i, label in enumerate(line['labels']):\n",
    "        line['labels'][i] = label2id[label]\n",
    "    line['label_description'] = label_desc[label_text]\n",
    "\n",
    "with open(f'{state}/train.json', 'w') as f:\n",
    "    json.dump(shuffle_train, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f'{state}/test.json', 'w') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label description\n",
    "with open('label-description-with-example.jsonl') as f:\n",
    "    label_desc = f.readlines()\n",
    "    label_desc = list(map(json.loads, label_desc))\n",
    "    label_desc = {line['label']: line['description'] for line in label_desc}\n",
    "\n",
    "for line in shuffle_train:\n",
    "    label_text = id2label[line['labels'][0]]\n",
    "    line['label_description'] = label_desc[label_text]\n",
    "\n",
    "for line in test:\n",
    "    label_text = id2label[line['labels'][0]]\n",
    "    line['label_description'] = label_desc[label_text]\n",
    "\n",
    "with open(f'{state}/train-with-example.json', 'w') as f:\n",
    "    json.dump(shuffle_train, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f'{state}/test-with-example.json', 'w') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
